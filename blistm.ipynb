{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8556247-f662-4253-ac6f-2eff5e882f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\wst20\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.828 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "C:\\Users\\wst20\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wst20\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">7,840,896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ attention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">628</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m7,840,896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │          \u001b[38;5;34m98,816\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ attention (\u001b[38;5;33mAttention\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m628\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,948,661</span> (30.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,948,661\u001b[0m (30.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,948,661</span> (30.32 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,948,661\u001b[0m (30.32 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m336/563\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 186ms/step - accuracy: 0.5002 - loss: 0.6963"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# -------------------------------\n",
    "# 1. 数据读取函数（正负样本）\n",
    "# -------------------------------\n",
    "def read_review_file(file_path, label, encoding=\"utf-8\"):\n",
    "    with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\n",
    "        data = f.read()\n",
    "    pattern = re.compile(r'<review.*?>(.*?)</review>', re.S)\n",
    "    matches = pattern.findall(data)\n",
    "    texts = []\n",
    "    for text in matches:\n",
    "        text_clean = \"\\n\".join([line.strip() for line in text.strip().splitlines() if line.strip()])\n",
    "        if text_clean:\n",
    "            texts.append(text_clean)\n",
    "    labels = [label] * len(texts)\n",
    "    return texts, labels\n",
    "\n",
    "# -------------------------------\n",
    "# 2. 中文分词函数\n",
    "# -------------------------------\n",
    "def chinese_tokenize(text):\n",
    "    return \" \".join(jieba.lcut(text))\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Attention 层\n",
    "# -------------------------------\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\")\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x: [batch_size, timesteps, features]\n",
    "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        a = tf.keras.backend.softmax(e, axis=1)\n",
    "        output = tf.keras.backend.sum(a * x, axis=1)\n",
    "        return output\n",
    "\n",
    "# -------------------------------\n",
    "# 4. 准备训练数据（示例路径，修改为你的训练集路径）\n",
    "# -------------------------------\n",
    "cn_neg_file = r\"D:\\zuoye\\商品评价文本情感分类\\训练集\\evaltask2_sample_data\\cn_sample_data\\sample.negative.txt\"\n",
    "cn_pos_file = r\"D:\\zuoye\\商品评价文本情感分类\\训练集\\evaltask2_sample_data\\cn_sample_data\\sample.positive.txt\"\n",
    "en_neg_file = r\"D:\\zuoye\\商品评价文本情感分类\\训练集\\evaltask2_sample_data\\en_sample_data\\sample.negative.txt\"\n",
    "en_pos_file = r\"D:\\zuoye\\商品评价文本情感分类\\训练集\\evaltask2_sample_data\\en_sample_data\\sample.positive.txt\"\n",
    "\n",
    "cn_neg_text, cn_neg_label = read_review_file(cn_neg_file, 0, \"utf-8\")\n",
    "cn_pos_text, cn_pos_label = read_review_file(cn_pos_file, 1, \"utf-8\")\n",
    "en_neg_text, en_neg_label = read_review_file(en_neg_file, 0, \"latin1\")\n",
    "en_pos_text, en_pos_label = read_review_file(en_pos_file, 1, \"latin1\")\n",
    "\n",
    "train_texts = cn_neg_text + cn_pos_text + en_neg_text + en_pos_text\n",
    "train_labels = cn_neg_label + cn_pos_label + en_neg_label + en_pos_label\n",
    "\n",
    "# 中文分词\n",
    "train_texts = [chinese_tokenize(t) for t in train_texts]\n",
    "\n",
    "# -------------------------------\n",
    "# 5. 文本序列化\n",
    "# -------------------------------\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "max_len = 500  # 限制最大长度\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "labels = np.array(train_labels)\n",
    "\n",
    "# 划分训练/验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.1, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# -------------------------------\n",
    "# 6. 构建 BiLSTM + Attention 模型\n",
    "# -------------------------------\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len)(input_layer)\n",
    "bilstm = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
    "attention = Attention()(bilstm)\n",
    "dense1 = Dense(64, activation='relu')(attention)\n",
    "dropout = Dropout(0.5)(dense1)\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# 7. 模型训练\n",
    "# -------------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 8. 测试集评估函数\n",
    "# -------------------------------\n",
    "def read_test_dataset(file_path, encoding=\"utf-8\"):\n",
    "    with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\n",
    "        data = f.read()\n",
    "    pattern = re.compile(r'<review id=\"(\\d+)\"\\s+label=\"(\\d+)\">(.*?)</review>', re.S)\n",
    "    matches = pattern.findall(data)\n",
    "    texts, labels = [], []\n",
    "    for _, label, text in matches:\n",
    "        text_clean = \"\\n\".join([line.strip() for line in text.strip().splitlines() if line.strip()])\n",
    "        texts.append(chinese_tokenize(text_clean))\n",
    "        labels.append(int(label))\n",
    "    return texts, np.array(labels)\n",
    "\n",
    "# 示例：测试集\n",
    "cn_file = r\"D:\\zuoye\\商品评价文本情感分类\\测试集标注\\Sentiment Classification with Deep Learning\\test.label.cn.txt\"\n",
    "en_file = r\"D:\\zuoye\\商品评价文本情感分类\\测试集标注\\Sentiment Classification with Deep Learning\\test.label.en.txt\"\n",
    "\n",
    "test_cn_texts, test_cn_labels = read_test_dataset(cn_file, \"utf-8\")\n",
    "test_en_texts, test_en_labels = read_test_dataset(en_file, \"utf-8\")\n",
    "\n",
    "test_cn_seq = pad_sequences(tokenizer.texts_to_sequences(test_cn_texts), maxlen=max_len, padding='post')\n",
    "test_en_seq = pad_sequences(tokenizer.texts_to_sequences(test_en_texts), maxlen=max_len, padding='post')\n",
    "\n",
    "# 评估\n",
    "def evaluate_model(test_seq, test_labels, lang=\"CN\"):\n",
    "    preds = (model.predict(test_seq) > 0.5).astype(int)\n",
    "    print(f\"\\n===== {lang} 测试集分类性能 =====\")\n",
    "    print(confusion_matrix(test_labels, preds))\n",
    "    print(classification_report(test_labels, preds))\n",
    "\n",
    "evaluate_model(test_cn_seq, test_cn_labels, \"CN\")\n",
    "evaluate_model(test_en_seq, test_en_labels, \"EN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa52c1-e615-4a37-92b5-2b52bf2c8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy 曲线\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Loss 曲线\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4dd9a-49d8-4c24-86b3-95f7d40b61e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion(test_seq, test_labels, lang=\"CN\"):\n",
    "    preds = (model.predict(test_seq) > 0.5).astype(int)\n",
    "    cm = confusion_matrix(test_labels, preds)\n",
    "    plt.figure(figsize=(4,3))\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "    plt.title(f\"Confusion Matrix - {lang} Test Set\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion(test_cn_seq, test_cn_labels, \"CN\")\n",
    "plot_confusion(test_en_seq, test_en_labels, \"EN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c32cb-39a6-4936-98ea-ae1fb5cc5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def plot_prf(test_seq, test_labels, lang=\"CN\"):\n",
    "    preds = (model.predict(test_seq) > 0.5).astype(int)\n",
    "    p = precision_score(test_labels, preds)\n",
    "    r = recall_score(test_labels, preds)\n",
    "    f = f1_score(test_labels, preds)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar([\"Precision\", \"Recall\", \"F1-score\"], [p, r, f])\n",
    "    plt.title(f\"PRF Metrics - {lang} Test Set\")\n",
    "    for i, v in enumerate([p, r, f]):\n",
    "        plt.text(i, v, f\"{v:.3f}\", ha='center', va='bottom')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "plot_prf(test_cn_seq, test_cn_labels, \"CN\")\n",
    "plot_prf(test_en_seq, test_en_labels, \"EN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcc7f2-1ec7-4aeb-bd51-ba6bc9cecab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_attention(text, tokenizer, model, max_len=200):\n",
    "    \"\"\"\n",
    "    text: 输入文本（字符串中文或英文均可）\n",
    "    tokenizer: 用于将文本转为序列的分词器\n",
    "    model: 训练好的深度学习模型（要求 forward 返回 attention 权重）\n",
    "    \"\"\"\n",
    "    # 1. 文本 -> token 序列\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    seq = tokenizer.pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    # 2. 模型前向传递，输出 attention 权重\n",
    "    with torch.no_grad():\n",
    "        output, attention_weights = model(seq)   # 模型需返回 attention\n",
    "\n",
    "    # 3. 将 token ID 还原为文字\n",
    "    tokens = tokenizer.sequences_to_texts(seq)[0].split()\n",
    "\n",
    "    # attention 权重 shape: [batch, seq_len]\n",
    "    att = attention_weights.squeeze().cpu().numpy()\n",
    "    att = att[:len(tokens)]     # 去掉 padding 位置\n",
    "\n",
    "    # 4. 可视化\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    plt.bar(range(len(tokens)), att)\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=45, fontsize=10)\n",
    "    plt.title(\"Attention Weights Heatmap\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255aca46-2ecc-413c-a259-3e6838ca037f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch210)",
   "language": "python",
   "name": "torch210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
